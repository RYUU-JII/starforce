"""
CLI ì§„ì…ì : í¬ë¡¤ë§ + ë°ì´í„° ì²˜ë¦¬ + ë¶„ì„ í†µí•©
"""
import argparse
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from crawler.config import DATA_DIR
from crawler.nexon_crawler import run_crawler
from crawler.scheduler import run_scheduler
from crawler.data_processor import DeltaCalculator, SessionManager
from crawler.manipulation_detector import ManipulationDetector, run_analysis


# ì„¸ì…˜ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬
SESSIONS_DIR = DATA_DIR.parent / "sessions"


def crawl_and_process(headless: bool = True) -> dict:
    """í¬ë¡¤ë§ + ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰ (ìë™ íŒ¨ì¹˜ ê°ì§€)"""
    # 1. í¬ë¡¤ë§
    result = asyncio.run(run_crawler(headless=headless))
    
    # 2. ì„¸ì…˜ ê´€ë¦¬
    session_mgr = SessionManager(SESSIONS_DIR)
    session_dir = session_mgr.get_current_session()
    
    # 3. Delta ê³„ì‚° (ë¦¬ì…‹ ê°ì§€ í¬í•¨)
    processor = DeltaCalculator(session_dir)
    deltas, reset_detected = processor.process_crawl_result(result)
    
    # 4. ë¦¬ì…‹ ê°ì§€ ì‹œ ìƒˆ ì„¸ì…˜ ì‹œì‘
    if reset_detected:
        print(f"\nğŸ”„ íŒ¨ì¹˜ ê°ì§€! ìƒˆ ì„¸ì…˜ì„ ìë™ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
        new_session_name = datetime.now().strftime("patch_%Y%m%d_%H%M")
        session_dir = session_mgr.start_new_session(new_session_name)
        
        # ìƒˆ ì„¸ì…˜ì— í˜„ì¬ ë°ì´í„°ë¥¼ ì²« ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì €ì¥
        new_processor = DeltaCalculator(session_dir)
        new_processor.process_crawl_result(result)
        
        print(f"âœ… ìƒˆ ì„¸ì…˜ ìƒì„±: {new_session_name}")
    
    print(f"\n{'='*50}")
    print(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
    print(f"{'='*50}")
    print(f"ì„¸ì…˜: {session_dir.name}")
    print(f"ìƒˆë¡œìš´ Delta: {len(deltas)}ê°œ")
    if reset_detected:
        print(f"âš ï¸ íŒ¨ì¹˜ ë¦¬ì…‹ ê°ì§€ë¡œ ì¸í•´ ìƒˆ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return result



def analyze_current_session():
    """í˜„ì¬ ì„¸ì…˜ ë¶„ì„"""
    session_mgr = SessionManager(SESSIONS_DIR)
    session_dir = session_mgr.get_current_session()
    
    print(f"\n{'='*50}")
    print(f"ì¡°ì‘ íƒì§€ ë¶„ì„ ì‹œì‘")
    print(f"ì„¸ì…˜: {session_dir.name}")
    print(f"{'='*50}")
    
    summary = run_analysis(session_dir)
    
    print(f"\në¶„ì„ ê²°ê³¼:")
    print(f"  - ë¶„ì„ëœ ì„±/ì´ë²¤íŠ¸ ì¡°í•©: {summary.total_star_levels_analyzed}")
    print(f"  - ì˜ì‹¬ í•­ëª© ìˆ˜: {summary.suspicious_count}")
    print(f"  - ì „ì²´ ì˜ì‹¬ ì ìˆ˜: {summary.overall_suspicion_score}/100")
    print(f"\ní•´ì„: {summary.interpretation}")
    
    return summary


def new_session(name: str = None):
    """ìƒˆ íŒ¨ì¹˜ ì„¸ì…˜ ì‹œì‘"""
    session_mgr = SessionManager(SESSIONS_DIR)
    
    if name is None:
        name = datetime.now().strftime("patch_%Y%m%d")
    
    session_dir = session_mgr.start_new_session(name)
    print(f"ìƒˆ ì„¸ì…˜ ì‹œì‘: {session_dir}")
    return session_dir


def list_sessions():
    """ëª¨ë“  ì„¸ì…˜ ëª©ë¡"""
    session_mgr = SessionManager(SESSIONS_DIR)
    sessions = session_mgr.list_sessions()
    
    print(f"\n{'='*50}")
    print(f"íŒ¨ì¹˜ ì„¸ì…˜ ëª©ë¡")
    print(f"{'='*50}")
    
    for i, s in enumerate(sessions, 1):
        status = "ğŸŸ¢ í™œì„±" if s.get("active") else "âšª ì¢…ë£Œ"
        print(f"{i}. {s['name']} {status}")
        print(f"   ì‹œì‘: {s['start_date']}")
        if s.get("end_date"):
            print(f"   ì¢…ë£Œ: {s['end_date']}")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="ë„¥ìŠ¨ ë‚˜ìš° ìŠ¤íƒ€í¬ìŠ¤ ë°ì´í„° í¬ë¡¤ëŸ¬ ë° ë¶„ì„ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python -m crawler.main                    # 1íšŒ í¬ë¡¤ë§ + ì²˜ë¦¬ (ë¸Œë¼ìš°ì € í‘œì‹œ)
  python -m crawler.main --headless         # 1íšŒ í¬ë¡¤ë§ + ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)
  python -m crawler.main --schedule         # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ (1ì‹œê°„ë§ˆë‹¤)
  python -m crawler.main --analyze          # í˜„ì¬ ì„¸ì…˜ ë¶„ì„
  python -m crawler.main --new-session      # ìƒˆ íŒ¨ì¹˜ ì„¸ì…˜ ì‹œì‘
  python -m crawler.main --list-sessions    # ì„¸ì…˜ ëª©ë¡ ë³´ê¸°
        """
    )
    
    parser.add_argument(
        "--headless", 
        action="store_true",
        help="ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰"
    )
    
    parser.add_argument(
        "--schedule", 
        action="store_true",
        help="ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ (1ì‹œê°„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§)"
    )
    
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="í˜„ì¬ ì„¸ì…˜ ë°ì´í„° ë¶„ì„ (ì¡°ì‘ íƒì§€)"
    )
    
    parser.add_argument(
        "--new-session",
        nargs="?",
        const="",
        metavar="NAME",
        help="ìƒˆ íŒ¨ì¹˜ ì„¸ì…˜ ì‹œì‘ (ì„ íƒì  ì´ë¦„ ì§€ì •)"
    )
    
    parser.add_argument(
        "--list-sessions",
        action="store_true",
        help="ëª¨ë“  ì„¸ì…˜ ëª©ë¡ ë³´ê¸°"
    )
    
    args = parser.parse_args()
    
    if args.list_sessions:
        list_sessions()
    elif args.new_session is not None:
        name = args.new_session if args.new_session else None
        new_session(name)
    elif args.analyze:
        analyze_current_session()
    elif args.schedule:
        run_scheduler()
    else:
        # ê¸°ë³¸: 1íšŒ í¬ë¡¤ë§ + ì²˜ë¦¬
        result = crawl_and_process(headless=args.headless)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*50}")
        print(f"í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*50}")
        print(f"ìˆ˜ì§‘ ì‹œê°„: {result['crawled_at']}")
        print(f"í™•ë¥  ë°ì´í„°: {result['summary']['total_prob_responses']}ê°œ")


if __name__ == "__main__":
    main()
